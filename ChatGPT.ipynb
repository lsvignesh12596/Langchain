{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNpKQdY6qke6usQ7Ashng8C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lsvignesh12596/Langchain-and-GPT/blob/master/ChatGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Directly through Open AI APIs"
      ],
      "metadata": {
        "id": "vsHLEPbk0OqW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ref Link - https://beta.openai.com/docs/api-reference/embeddings/create?lang=python"
      ],
      "metadata": {
        "id": "fMCqUQMOBK66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "jenL4Siw0NZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install gradio"
      ],
      "metadata": {
        "id": "Eo7QHloAG5Cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "b1IryOHEGP4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "api_key = \"\""
      ],
      "metadata": {
        "id": "VPFsn19dzdUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "openai.organization = \"org-bg1rw1nozg0jPk3ogfGSunXR\"\n",
        "# openai.api_key = os.getenv(\"openai_api_key\")\n",
        "openai.api_key = api_key\n"
      ],
      "metadata": {
        "id": "3gMvaU730Wik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# List of OpenAI Models"
      ],
      "metadata": {
        "id": "9jJtCbKirSFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all open ai models\n",
        "result = openai.Model.list()\n",
        "models = []\n",
        "for i in range(0,len(result.data)):\n",
        "  models.append(result.data[i].root)\n",
        "models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kV0ZWucawp3Y",
        "outputId": "b685127a-306a-49c8-f46e-0679c78a5a01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['babbage',\n",
              " 'davinci',\n",
              " 'text-davinci-edit-001',\n",
              " 'babbage-code-search-code',\n",
              " 'text-similarity-babbage-001',\n",
              " 'code-davinci-edit-001',\n",
              " 'text-davinci-001',\n",
              " 'ada',\n",
              " 'text-davinci-003',\n",
              " 'babbage-code-search-text',\n",
              " 'babbage-similarity',\n",
              " 'code-search-babbage-text-001',\n",
              " 'text-curie-001',\n",
              " 'whisper-1',\n",
              " 'code-search-babbage-code-001',\n",
              " 'text-ada-001',\n",
              " 'text-embedding-ada-002',\n",
              " 'text-similarity-ada-001',\n",
              " 'curie-instruct-beta',\n",
              " 'ada-code-search-code',\n",
              " 'ada-similarity',\n",
              " 'code-search-ada-text-001',\n",
              " 'text-search-ada-query-001',\n",
              " 'davinci-search-document',\n",
              " 'ada-code-search-text',\n",
              " 'text-search-ada-doc-001',\n",
              " 'davinci-instruct-beta',\n",
              " 'text-similarity-curie-001',\n",
              " 'code-search-ada-code-001',\n",
              " 'ada-search-query',\n",
              " 'text-search-davinci-query-001',\n",
              " 'curie-search-query',\n",
              " 'davinci-search-query',\n",
              " 'babbage-search-document',\n",
              " 'ada-search-document',\n",
              " 'text-search-curie-query-001',\n",
              " 'text-search-babbage-doc-001',\n",
              " 'curie-search-document',\n",
              " 'text-search-curie-doc-001',\n",
              " 'babbage-search-query',\n",
              " 'text-babbage-001',\n",
              " 'text-search-davinci-doc-001',\n",
              " 'text-search-babbage-query-001',\n",
              " 'curie-similarity',\n",
              " 'curie',\n",
              " 'text-similarity-davinci-001',\n",
              " 'text-davinci-002',\n",
              " 'gpt-3.5-turbo-0301',\n",
              " 'gpt-3.5-turbo',\n",
              " 'davinci-similarity',\n",
              " 'cushman:2020-05-03',\n",
              " 'ada:2020-05-03',\n",
              " 'babbage:2020-05-03',\n",
              " 'curie:2020-05-03',\n",
              " 'davinci:2020-05-03',\n",
              " 'if-davinci-v2',\n",
              " 'if-curie-v2',\n",
              " 'if-davinci:3.0.0',\n",
              " 'davinci-if:3.0.0',\n",
              " 'davinci-instruct-beta:2.0.0',\n",
              " 'text-ada:001',\n",
              " 'text-davinci:001',\n",
              " 'text-curie:001',\n",
              " 'text-babbage:001']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Completion"
      ],
      "metadata": {
        "id": "dZoWzdrirVvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Completion\n",
        "result = openai.Completion.create(\n",
        "  model=\"text-davinci-003\",\n",
        "  prompt=\"Hulk is approached by a mysterious stranger in the tavern for a new quest.\",\n",
        "  echo=True,\n",
        "  max_tokens=1024,\n",
        "  temperature=0.2\n",
        ")\n",
        "print(\"Total No of Words - \", result.usage.total_tokens)\n",
        "pprint(result.choices[0].text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6hI0cBx3pxS",
        "outputId": "1e4d4f5a-9b50-4581-a8d7-06e6261819db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total No of Words -  64\n",
            "('Hulk is approached by a mysterious stranger in the tavern for a new quest.\\n'\n",
            " '\\n'\n",
            " 'The stranger says, \"Hulk, I have a quest for you. I need you to retrieve a '\n",
            " 'powerful artifact from a dangerous location. It is said to be guarded by a '\n",
            " 'powerful creature. Are you up for the challenge?\"')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Generator"
      ],
      "metadata": {
        "id": "9R-pTdlmrYWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Image Creator\n",
        "result = openai.Image.create(\n",
        "  prompt=\"dog at grand canyon\",\n",
        "  n=2,\n",
        "  size=\"1024x1024\"\n",
        ")\n",
        "urls = []\n",
        "for i in range(len(result.data)):\n",
        "  urls.append(result.data[i].url)\n",
        "urls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnUvNwZD8Zh_",
        "outputId": "db6f09c3-f7ae-4ea6-df99-ac824da075f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['https://oaidalleapiprodscus.blob.core.windows.net/private/org-bg1rw1nozg0jPk3ogfGSunXR/user-ohPrQOS0KqGVtHq7wk5E6ojj/img-vFe6lqiMfvAiQBfMrwbAEEbR.png?st=2023-04-14T07%3A10%3A01Z&se=2023-04-14T09%3A10%3A01Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-04-13T17%3A15%3A13Z&ske=2023-04-14T17%3A15%3A13Z&sks=b&skv=2021-08-06&sig=JKiH9yj1QwS/QGD%2BW%2BFI8XJssLxYVqTW1vi5Nmro0lI%3D',\n",
              " 'https://oaidalleapiprodscus.blob.core.windows.net/private/org-bg1rw1nozg0jPk3ogfGSunXR/user-ohPrQOS0KqGVtHq7wk5E6ojj/img-Q6NWEbOPpq44s1v0hjPhSG1Z.png?st=2023-04-14T07%3A10%3A01Z&se=2023-04-14T09%3A10%3A01Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-04-13T17%3A15%3A13Z&ske=2023-04-14T17%3A15%3A13Z&sks=b&skv=2021-08-06&sig=2QS5hoYHVUjAiuKn%2BjMffwiXyHDG0BFmAe7wy1HT7Mk%3D']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wf8IvY4H_dXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Generator - WIP"
      ],
      "metadata": {
        "id": "CFP_lKaqrcb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwONYOIp16cB",
        "outputId": "7447c939-c81f-44eb-92e4-9e94a0ba8c68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.4-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.3/70.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.9/dist-packages (from openai) (2.27.1)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.9.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (269 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.3/269.3 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.4 yarl-1.9.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "_mGqGFOS1_7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# set up OpenAI API client\n",
        "openai.api_key = \"\""
      ],
      "metadata": {
        "id": "Da-_d-5z2B6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read in the Excel file\n",
        "df = pd.read_excel('/content/sales_data_sample - Copy.xlsx')"
      ],
      "metadata": {
        "id": "hDda-gBX2FOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function to prompt user for input and interpret using ChatGPT\n",
        "def prompt_user(prompt):\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"davinci\",\n",
        "        prompt=prompt,\n",
        "        max_tokens=50,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        temperature=0.5,\n",
        "    )\n",
        "    return response.choices[0].text.strip()"
      ],
      "metadata": {
        "id": "cVZb-b3D2jsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get user input for which columns to clean\n",
        "columns_to_clean = input(\"Which columns do you want to clean? \")\n",
        "\n",
        "# loop through each column and clean as needed\n",
        "for column in columns_to_clean.split(\",\"):\n",
        "    clean_method = prompt_user(\"How should column {} be cleaned? \".format(column))\n",
        "    response = prompt_user(\"What should be done to {} column?\".format(clean_method))\n",
        "    code_to_execute = response # This is the code snippet to execute\n",
        "    user_input = input(\"Please provide any input needed to execute the code for {} column: \".format(column))\n",
        "    exec(code_to_execute.format(user_input)) # Execute the code snippet with user input\n",
        "  \n",
        "    "
      ],
      "metadata": {
        "id": "WGqpXN0JHANs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "outputId": "69de0f18-3177-4dfc-bfd0-dfbedbf7c615"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Which columns do you want to clean? all\n",
            "Please provide any input needed to execute the code for all column: impute missing with mean\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-83220d1fea12>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mcode_to_execute\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;31m# This is the code snippet to execute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Please provide any input needed to execute the code for {} column: \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_to_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Execute the code snippet with user input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name '__________________________________________________' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "code_to_execute"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "IV3_nglt4lLs",
        "outputId": "4da2dbd2-0571-4d49-bdfc-da3387ab3b53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'__________________________________________________\\n\\n__________________________________________________\\n\\n__________________________________________________\\n\\n__________________________________________________\\n\\n__________________________________________________\\n\\n__________________________________________________\\n\\n__________________________________________________\\n\\n__________________________________________________\\n\\n__________________________________________________\\n\\n__________________________________________________'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# write cleaned data to new Excel file\n",
        "df.to_excel('cleaned_data.xlsx', index=False)"
      ],
      "metadata": {
        "id": "YqAniZNvHAQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6iofzjoBHAUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# set up OpenAI API client\n",
        "openai.api_key = \"\"\n",
        "\n",
        "# define a function to prompt user for input and interpret using ChatGPT\n",
        "def prompt_user(prompt):\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"davinci\",\n",
        "        prompt=prompt,\n",
        "        max_tokens=50,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        temperature=0.5,\n",
        "    )\n",
        "    return response.choices[0].text.strip()\n",
        "\n",
        "# prompt user for input on what code to generate\n",
        "code_prompt = input(\"What kind of code do you want to generate? \")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eF045W4-HEUR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b22eb89a-3a62-4b60-8526-70892c8e86bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What kind of code do you want to generate? generate code to create a bar graph on oil exported by russia in 2022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate code based on user input using ChatGPT\n",
        "response = openai.Completion.create(\n",
        "    engine=\"davinci\",\n",
        "    prompt=code_prompt,\n",
        "    max_tokens=1024,\n",
        "    n=1,\n",
        "    stop=None,\n",
        "    temperature=0.5,\n",
        ")\n",
        "\n",
        "# extract generated code from response\n",
        "generated_code = response.choices[0].text.strip()\n",
        "\n",
        "# execute generated code\n",
        "exec(generated_code)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "mIIie8Uz3jyI",
        "outputId": "cc0f4db1-bbf1-4c48-c558-7e8306a032c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
            "  File \u001b[1;32m\"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3553\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-74ed1de37433>\"\u001b[0;36m, line \u001b[0;32m15\u001b[0;36m, in \u001b[0;35m<cell line: 15>\u001b[0;36m\u001b[0m\n\u001b[0;31m    exec(generated_code)\u001b[0m\n",
            "\u001b[0;36m  File \u001b[0;32m\"<string>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    .\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_code"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "HVO_NQ4z3-hk",
        "outputId": "d8b77579-41e0-496b-8fa6-a1129c0acb83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'.\\n\\nI have a data frame with a list of numbers and a column with the names of those numbers. I need to create a bar graph with the names of the numbers on the x-axis and the numbers on the y-axis.\\n\\nI have tried the following code but it didn\\'t work.\\n\\ndata <- read.csv(\"OilExports.csv\") #data has the following columns: #1. Date #2. Oil Exported (in barrels) data$Date <- as.Date(data$Date, format=\"%m/%d/%Y\") #data$Date is a list of numbers. data$Date <- as.numeric(data$Date) #data$Date is a list of numbers. data$Date <- as.Date(data$Date, format=\"%m/%d/%Y\") #data$Date is a list of numbers. data$Date <- as.numeric(data$Date) #data$Date is a list of numbers. data$Date <- as.Date(data$Date, format=\"%m/%d/%Y\") #data$Date is a list of numbers. data$Date <- as.numeric(data$Date) #data$Date is a list of numbers. data$Date <- as.Date(data$Date, format=\"%m/%d/%Y\") #data$Date is a list of numbers. data$Date <- as.numeric(data$Date) #data$Date is a list of numbers. #data$Date <- as.Date(data$Date, format=\"%m/%d/%Y\") #data$Date is a list of numbers. #data$Date <- as.numeric(data$Date) #data$Date is a list of numbers. #data$Date <- as.Date(data$Date, format=\"%m/%d/%Y\") #data$Date is a list of numbers. #data$Date <- as.numeric(data$Date) #data$Date is a list of numbers. #data$Date <- as.Date(data$Date, format=\"%m/%d/%Y\") #data$Date is a list of numbers. #data$Date <- as.numeric(data$Date) #data$Date is a list of numbers. #data$Date <- as.Date(data$Date, format=\"%m/%d/%Y\") #data$Date is a list of numbers. #data$Date <- as.numeric(data$Date) #data$Date is a list of numbers. #data$Date <- as.Date(data$Date, format=\"%m/%d/%Y\") #data$Date is a list of numbers. #data$Date <- as.numeric(data$Date) #data$Date is a list of numbers. #data$Date <- as.Date(data$Date, format=\"%m/%d/%Y\") #data$Date is a list of numbers. #data$Date <- as.numeric(data$Date) #data$Date is a list of numbers. #data$Date <- as.Date(data$Date, format=\"%m/%d/%Y\") #data$Date is a list of numbers. #data$Date <- as.numeric(data$Date) #data$Date is a list of numbers. #data$Date <- as.Date(data$Date, format=\"%m/%d/%Y\") #data$Date is a list of numbers. #data$Date <- as.numeric(data$Date) #data$Date is a list of numbers. #data$Date <- as.Date(data$Date, format=\"%m/%d/%Y\") #data$Date is a list of numbers. #data$Date <- as.numeric(data$Date) #data$Date is a list of numbers. #data$Date <- as.Date(data$Date, format=\"%m/%d/%Y\") #data$Date is a list of numbers. #data$Date <- as.numeric(data$Date) #data$Date is a list of numbers. #data$Date <- as.Date(data$Date, format=\"%m/%d/%Y\") #data$Date is a list of numbers. #data$Date <- as.numeric(data$Date) #data$Date is a list of numbers. #data$Date <- as.Date(data$Date, format=\"%m/%d/%Y\") #data$Date is a list of numbers. #data$Date <- as.numeric(data$Date) #data$Date is a list of numbers. #data$Date <-'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def complete(prompt):\n",
        "    # query text-davinci-003\n",
        "    res = openai.Completion.create(\n",
        "        engine='text-davinci-003',\n",
        "        prompt=prompt,\n",
        "        temperature=0,\n",
        "        max_tokens=400,\n",
        "        top_p=1,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0,\n",
        "        stop=None\n",
        "    )\n",
        "    return res['choices'][0]['text'].strip()"
      ],
      "metadata": {
        "id": "3FACBRTa4bxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = (\"Code to use chatgpt api\")\n",
        "complete(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "6LJX08bC7ajK",
        "outputId": "6059bbbe-3d6a-4bbf-9e02-60ef41adefeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"//import the chatgpt library\\nconst chatgpt = require('chatgpt');\\n\\n//initialize the chatgpt instance\\nconst chatbot = new chatgpt();\\n\\n//start the conversation\\nchatbot.startConversation()\\n  .then(response => {\\n    console.log(response);\\n  })\\n  .catch(err => {\\n    console.log(err);\\n  });\\n\\n//send a message to the chatbot\\nchatbot.sendMessage('Hello')\\n  .then(response => {\\n    console.log(response);\\n  })\\n  .catch(err => {\\n    console.log(err);\\n  });\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LuoXA55t7r-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataFrame Analyzer - Approach1"
      ],
      "metadata": {
        "id": "JllRsTLPqq0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "BIY0fQaKE0k3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "cgvXbIeQE9U0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = \"\""
      ],
      "metadata": {
        "id": "IxfgckWUFO_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_answer(df, user_prompt):\n",
        "  text =  \" \".join(df.astype(str).agg(\" \".join, axis=1).tolist())\n",
        "  # print(text)\n",
        "\n",
        "  prompt = f\"{user_prompt} : {text}\"\n",
        "\n",
        "  response = openai.Completion.create(engine = \"text-davinci-003\", #\"text-davinci-002\",\n",
        "                                      prompt = prompt,\n",
        "                                      max_tokens = 128,\n",
        "                                      n = 1,\n",
        "                                      stop = None,\n",
        "                                      temperature = 0)\n",
        "  result = response.choices[0].text.strip()\n",
        "  return response, result"
      ],
      "metadata": {
        "id": "-RvIYlmFFi4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({\n",
        "    \"Name\" : [\"Alice\", \"Bob\", \"James\"],\n",
        "    \"Age\" : [32, 25, 56],\n",
        "    \"City\" : [\"New York\", \"Sydney\", \"Delhi\"]\n",
        "})"
      ],
      "metadata": {
        "id": "1PbTKtlGFfmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.DataFrame({'ORDERNUMBER': [10107,10121, 10134, 10145, 10159, 10168, 10180, 10188, 10201, 10211],  \n",
        "                     'QUANTITYORDERED': [30, 34, 41, 45, 49, 36, 29, 48, 22, 41],  \n",
        "                     'PRICE': [95.7, 81.35, 94.74, 83.26, 100, 96.66, 86.13, 100, 98.57, 100],  \n",
        "                     'ORDERLINENUMBER': [2, 5, 2, 6, 14, 1, 9, 1, 2, 14],  \n",
        "                     # 'SALES': [2871, 2765.9, 3884.34, 3746.7, 5205.27, 3479.76, 2497.77, 5512.32, 2168.54, 4708.44],  \n",
        "                     'ORDERDATE': ['2/24/2003 0:00', '05-07-2003 00:00', '07-01-2003 00:00', '8/25/2003 0:00', '10-10-2003 00:00', \n",
        "                                   '10/28/2003 0:00', '11-11-2003 00:00', '11/18/2003 0:00', '12-01-2003 00:00', '1/15/2004 0:00'],  \n",
        "                     'STATUS': ['Shipped', 'Shipped', 'Shipped', 'Shipped', 'Shipped', 'Shipped', 'Not Shipped', 'Shipped', 'Shipped', 'Shipped'],  \n",
        "                     'QTR_ID': [1, 2, 3, 3, 4, 4, 4, 4, 4, 1],  'MONTH_ID': [2, 5, 7, 8, 10, 10, 11, 11, 12, 1],  \n",
        "                     'YEAR': [2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2004],  \n",
        "                     'PRODUCTLINE': ['Motorcycles', 'Coolers', 'Motorcycles', 'Motorcycles', 'Coolers', 'Motorcycles', \n",
        "                                     'Bikes', 'Motorcycles', 'Bikes', 'Motorcycles'],  \n",
        "                     'MSRP': [95, 95, 95, 95, 95, 95, 95, 95, 95, 95]})"
      ],
      "metadata": {
        "id": "CYbgv5tpHzX9"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get_answer(df1, \"What are the different products sold?\")\n",
        "# get_answer(df1, \"how many products have not been shipped?\")\n",
        "response, result = get_answer(df1, \"how many products have not been shipped? Use the Status column.\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TblHW6OTHg32",
        "outputId": "1fc845e4-ac91-4bf3-9d0f-dc0aeeb24e18"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ggMYNM6aJ5UL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ttTohV5hMSm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataFrame Analyzer - Approach2"
      ],
      "metadata": {
        "id": "0R9e-AzONxp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install python-dotenv\n",
        "!pip install youtube-transcript-api\n",
        "!pip install tiktoken\n",
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "yTVvjwOfMSjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import YoutubeLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from dotenv import find_dotenv, load_dotenv\n",
        "from langchain.prompts.chat import(\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate\n",
        ")\n",
        "import textwrap"
      ],
      "metadata": {
        "id": "oDiPyxbzMSgb"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = \"\""
      ],
      "metadata": {
        "id": "31HEv0r8MSc7"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_dotenv(find_dotenv())\n",
        "embeddings = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "MOncpHSRN4FX"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_db_from_df(df):\n",
        "    # loader = YoutubeLoader.from_youtube_url(url)\n",
        "    transcript = \" \".join(df.astype(str).agg(\"\\n\".join, axis=1).tolist())\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap=100)\n",
        "    texts = text_splitter.create_documents([transcript])\n",
        "    # docs = text_splitter.split_documents(transcript)\n",
        "\n",
        "    db = FAISS.from_documents(texts, embeddings)\n",
        "    return db"
      ],
      "metadata": {
        "id": "BEEeoEeRN4Bt"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_response_from_query(db, query, k=4):\n",
        "  \"\"\"\n",
        "  gpt-3.5-turbo can handle upto 4097 tokens. \n",
        "  Setting the chunk size to 1000 and k = 4 maximizes\n",
        "  the number of tokens to analyze\n",
        "  \"\"\"\n",
        "\n",
        "  docs = db.similarity_search(query, k=k)\n",
        "  docs_page_content = \" \".join([d.page_content for d in docs])\n",
        "\n",
        "  chat = ChatOpenAI(model_name = \"gpt-3.5-turbo\", temperature = 0.2)\n",
        "\n",
        "  # Template to use for the system message prompt\n",
        "  template = \"\"\"\n",
        "  You are a helpful assistant that can answer questions about the dataframe passed : {docs}\n",
        "\n",
        "  Only use the factual information from the text to answer the question.\n",
        "\n",
        "  If you feel there isn't enough information to answer the question, say \"I do not have answer to this question\".\n",
        "\n",
        "  Your answer should be precise.\n",
        "\n",
        "  You might use sql queries to fetch the data based on the query and then answer the query.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  sys_msg_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "\n",
        "  # human question prompt\n",
        "  human_template = \"Answer the following question: {question}\"\n",
        "  human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "  chat_prompt = ChatPromptTemplate.from_messages(\n",
        "      [sys_msg_prompt, human_message_prompt]\n",
        "  )\n",
        "\n",
        "  chain = LLMChain(llm=chat, prompt = chat_prompt)\n",
        "\n",
        "  response = chain.run(question = query, docs = docs_page_content)\n",
        "  response = response.replace(\"\\n\", \"\")\n",
        "  return response, docs"
      ],
      "metadata": {
        "id": "itYtTS9HN3-V"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db1 = create_db_from_df(df)"
      ],
      "metadata": {
        "id": "pWOEHhy1fU7U"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the age of James staying in the city of New York?\"\n",
        "response, docs = get_response_from_query(db1, query)\n",
        "print(textwrap.fill(response, width = 85))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWgzg4VwfX0u",
        "outputId": "bd70b3c1-a7f1-47fb-9034-d2fefa5b6f04"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The age of James staying in the city of New York is not specified in the given\n",
            "dataframe.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "db = create_db_from_df(df1)"
      ],
      "metadata": {
        "id": "Nedp2pkGN37U"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What does the data represent?\"\n",
        "response, docs = get_response_from_query(db, query)\n",
        "print(textwrap.fill(response, width = 85))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyAdVroMN34D",
        "outputId": "2a69d337-bccb-4706-fd2b-8163c8956c42"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The data represents a dataframe that contains information about various orders made\n",
            "by customers. Each row in the dataframe represents a different order and contains\n",
            "details such as the order number, the quantity of items ordered, the price per item,\n",
            "the order date, the status of the order (shipped or not shipped), the month and year\n",
            "of the order, and the product category. The data also includes some summary\n",
            "statistics such as the total number of orders and the total revenue generated from\n",
            "all orders.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What are the unique product categories?\"\n",
        "response, docs = get_response_from_query(db, query)\n",
        "print(textwrap.fill(response, width = 85))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4i2avfGN30T",
        "outputId": "24a0e757-c92b-4a25-ae53-44c964a25ba2"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The unique product categories are Motorcycles, Coolers, and Bikes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the price of the product sold in the year 2004?\"\n",
        "response, docs = get_response_from_query(db, query)\n",
        "print(textwrap.fill(response, width = 85))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wggz2gWWWuWf",
        "outputId": "30e80aea-6fd1-4819-83d0-60911e298ed4"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The price of the product sold in the year 2004 is not specified in the given text.\n",
            "Therefore, I do not have an answer to this question.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the cost of the product sold in the year 2004? Use the Year and Price column to answer this question.\"\n",
        "response, docs = get_response_from_query(db, query)\n",
        "print(textwrap.fill(response, width = 85))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhUBUXkQaF9h",
        "outputId": "cc66ebb7-c4c4-4acf-ef13-932ce47a35ba"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the information provided in the text, we can see that there are multiple\n",
            "products sold in different years. However, we do not have the complete information\n",
            "about all the products sold in the year 2004. We only have information about one\n",
            "product sold in the year 2004, which is a Motorcycle with a product code of 95 and a\n",
            "price of 10121. Therefore, the cost of the product sold in the year 2004 is 10121.\n",
            "However, if we want to find the total cost of all the products sold in the year 2004,\n",
            "we do not have enough information to answer this question. We do not have the\n",
            "complete information about all the products sold in the year 2004, such as their\n",
            "prices and product codes. Therefore, we cannot calculate the total cost of all the\n",
            "products sold in the year 2004.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the cost of the product sold in the year 2004? Use the Year and Price column to answer this question. Do not use Sales column.\"\n",
        "response, docs = get_response_from_query(db, query)\n",
        "print(textwrap.fill(response, width = 85))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iN75O76tWuOe",
        "outputId": "fa27d238-cb88-4778-c571-0a1be2c31ee0"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the information provided in the text, we can see that there is a \"Price\"\n",
            "column in the dataframe, which contains the cost of each product sold. Additionally,\n",
            "there is a \"Year\" column that indicates the year in which each product was sold.\n",
            "However, we do not have enough information to determine the cost of the product sold\n",
            "in the year 2004. The dataframe only contains information about products sold up\n",
            "until January 15, 2004. Therefore, we cannot determine the cost of products sold\n",
            "later in the year.In summary, we do not have enough information to answer this\n",
            "question.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the total price of the bikes purchased in year 2003? Use the PRODUCTLINE & PRICE column to calculate the total price.\"\n",
        "response, docs = get_response_from_query(db, query)\n",
        "print(textwrap.fill(response, width = 85))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYRGvgw-cMuV",
        "outputId": "ff42667b-1742-453f-9b31-67493109fce4"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We need to filter the rows where the PRODUCTLINE is \"Bikes\" and the year is 2003, and\n",
            "then sum up the prices in the PRICE column. However, the date format in the text is\n",
            "not consistent, so we need to convert them to a consistent format first. Let's assume\n",
            "that the date format is \"mm-dd-yyyy\".The rows with PRODUCTLINE \"Bikes\" and year 2003\n",
            "are:|    |   PRODUCTLINE   | PRICE | QUANTITYORDERED |    ORDERDATE    |   STATUS   |\n",
            "MONTH | YEAR | ORDERLINENUMBER ||----|----------------|-------|----------------|-----\n",
            "-----------|------------|-------|------|-----------------||  8 | Bikes          |\n",
            "100.0 |              1 | 11-18-2003 00:00 | Shipped    |    11 | 2003 |\n",
            "4 || 10 | Bikes          |  98.57 |              2 | 12-01-2003 00:00 | Shipped    |\n",
            "12 | 2003 |               4 |The total price is the sum of the PRICE\n",
            "column:total_price = 100.0 + 98.57 = 198.57Therefore, the total price of the bikes\n",
            "purchased in year 2003 is $198.57.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "3ZUuiKN8MTXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_answer(df, user_prompt):\n",
        "  text =  \" \".join(df.astype(str).agg(\" \".join, axis=1).tolist())\n",
        "  # print(text)\n",
        "\n",
        "  prompt = f\"{user_prompt} : {text}\"\n",
        "\n",
        "  response = openai.ChatCompletion.create(engine = \"text-davinci-003\", #\"text-davinci-002\",\n",
        "                                      prompt = prompt,\n",
        "                                      max_tokens = 128,\n",
        "                                      n = 1,\n",
        "                                      stop = None,\n",
        "                                      temperature = 0.1)\n",
        "  result = response.choices[0].text.strip()\n",
        "  return response, result"
      ],
      "metadata": {
        "id": "kzLjx95ZMUG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = \" \".join(df.astype(str).agg(\"\\r\\n\".join, axis=1).tolist())"
      ],
      "metadata": {
        "id": "sX2ut9kqMs97"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is a long document we can split up.\n",
        "# with open('../../../state_of_the_union.txt') as f:\n",
        "#     state_of_the_union = f.read()\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # Set a really small chunk size, just to show.\n",
        "    chunk_size = 100,\n",
        "    chunk_overlap  = 20,\n",
        "    # length_function = len,\n",
        ")\n",
        "texts = text_splitter.create_documents([t])\n",
        "print(texts)\n",
        "docs = text_splitter.split_documents(t)\n",
        "# print(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "Y1QJwJyZS6zN",
        "outputId": "c2e0fb90-8a55-40a5-8a9a-ad951760158d"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content='Alice\\r\\n32\\r\\nNew York Bob\\r\\n25\\r\\nSydney James\\r\\n56\\r\\nDelhi james\\r\\n23\\r\\nNew York', metadata={})]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-134-bb089e93da7d>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_splitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_splitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;31m# print(docs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/langchain/text_splitter.py\u001b[0m in \u001b[0;36msplit_documents\u001b[0;34m(self, documents)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msplit_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;34m\"\"\"Split documents.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mmetadatas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadatas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadatas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/langchain/text_splitter.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msplit_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;34m\"\"\"Split documents.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mmetadatas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadatas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadatas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'page_content'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yR3OQbIBULWb",
        "outputId": "fc00b82b-c4fd-4ee0-cc78-a50ef421ce72"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='95 10121\\r\\n34\\r\\n81.35\\r\\n5\\r\\n2765.9\\r\\n05-07-2003 00:00\\r\\nShipped\\r\\n2\\r\\n5\\r\\n2003\\r\\nCoolers\\r\\n95 10134\\r\\n41\\r\\n94.74', metadata={})"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pnQLnvtKU2J-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
